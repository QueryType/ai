# Dual Agent Chat

A lightweight, configuration-driven system for running conversations between two AI agents with advanced interactive controls.

## ğŸŒŸ Key Features

- **Memory-Efficient Architecture**: Single message list with role switching (50% memory savings)
- **Configuration-Driven**: All settings in JSON + Markdown files
- **OpenAI-Compatible**: Works with any OpenAI-compatible API (LM Studio, Ollama, etc.)
- **Interactive Control**: Press SPACE to interrupt and manually enter messages
- **Conversation Continuation**: Resume and extend existing conversations
- **Turn-by-Turn Viewer**: Review conversations interactively

## ğŸš€ Quick Start

### Installation

```bash
# Clone or navigate to the project
cd dual-agent-chat

# Install dependencies
pip install -r requirements.txt
```

### Configure Your LLM Server

Edit `app.json` to point to your LLM server:

```json
{
  "llm_config": {
    "api_base_url": "http://localhost:1234/v1",
    "model": "your-model-name"
  }
}
```

### Run Your First Conversation

```bash
# Start with the default Fantasy Tavern scenario
python main.py

# Use a different scenario
python main.py scenarios/debate/debate_config.json
```

## ğŸ’¡ How It Works

### The Innovation: Role Switching

Instead of maintaining two separate message histories (doubling memory), we use a single message list:

1. **System Prompt**: Dynamically built as `world_info + active_character_card`
2. **Role Flip**: All messages switch between `user` â†” `assistant` when speakers change
3. **Context Preserved**: Full conversation history maintained with 50% less memory

```
Turn 1 (Alice speaks):
  system: [World + Alice's card]
  assistant: "Hello!"

Turn 2 (Bob speaks):
  system: [World + Bob's card]    â† Changed!
  user: "Hello!"                   â† Role flipped!
  assistant: "Hi there."
```

## ğŸ® Advanced Features

### Interactive Interruption

Press SPACE during generation to take manual control:

```bash
python main.py --interrupt-timeout 10
```

When you see `â¸ï¸ Press SPACE to interrupt (10s timeout)...`, press SPACE to enter your own message. Type `auto` to resume automatic generation.

[Full Guide â†’](docs/FEATURES.md#interactive-interruption)

### Continue Conversations

Add more turns to existing conversations:

```bash
python main.py --continue conversations/my_chat.json --add-turns 5
```

[Full Guide â†’](docs/FEATURES.md#conversation-continuation)

### View Conversations

Interactive turn-by-turn viewing:

```bash
# List all conversations
python view_conversation.py

# View specific conversation interactively
python view_conversation.py 1 -i
```

[Full Guide â†’](docs/FEATURES.md#viewing-conversations)

## ğŸ“ Project Structure

```
dual-agent-chat/
â”œâ”€â”€ README.md                   # You are here
â”œâ”€â”€ main.py                     # Main application
â”œâ”€â”€ view_conversation.py        # Conversation viewer
â”œâ”€â”€ app.json                    # Default configuration
â”œâ”€â”€ config_loader.py            # Config handling
â”œâ”€â”€ message_manager.py          # Message list & role switching
â”œâ”€â”€ llm_client.py              # LLM API client
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ USAGE.md               # Comprehensive usage guide
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # Technical details
â”‚   â””â”€â”€ FEATURES.md            # All features explained
â”œâ”€â”€ scenarios/                  # Example scenarios
â”‚   â”œâ”€â”€ fantasy_tavern/        # Default: Medieval tavern
â”‚   â””â”€â”€ debate/                # Philosophy debate
â””â”€â”€ conversations/              # Saved conversations (auto-created)
```

## ğŸ“‹ Command Reference

### Basic Usage

```bash
# Default scenario
python main.py

# Custom scenario
python main.py scenarios/debate/debate_config.json

# Start in interactive mode
python main.py --interactive
```

### Continuation

```bash
# Continue with 5 more turns
python main.py --continue conversations/chat.json --add-turns 5

# Continue with custom timeout
python main.py --continue conversations/chat.json --add-turns 10 --interrupt-timeout 8
```

### Viewing

```bash
# List all conversations
python view_conversation.py

# View conversation #1
python view_conversation.py 1

# Interactive mode (press Enter for each turn)
python view_conversation.py 1 -i

# Export to markdown
python view_conversation.py 1 --export
```

### Options

```bash
--interrupt-timeout N     # Seconds to wait for SPACE interrupt (default: 5)
--interactive            # Start in fully interactive mode
--continue FILE          # Continue from existing conversation
--add-turns N            # Number of turns to add (with --continue)
```

## ğŸ­ Example Scenarios

### Fantasy Tavern (Default)

Two travelers meet in a medieval tavern:
- **Eldric**: A merchant storyteller
- **Mira**: A herbalist healer

```bash
python main.py
```

### Philosophical Debate

University students debate AI consciousness:
- **Alex**: Believes AI can achieve consciousness
- **Sam**: Skeptical of AI consciousness

```bash
python main.py scenarios/debate/debate_config.json
```

## ğŸ› ï¸ Creating Custom Scenarios

1. Create a scenario directory:
   ```bash
   mkdir -p scenarios/my_scenario
   ```

2. Create three markdown files:
   - `world.md` - Setting, context, rules
   - `character_a.md` - First character's card
   - `character_b.md` - Second character's card

3. Create `my_scenario_config.json`:
   ```json
   {
     "scenario_name": "My Scenario",
     "world_info_file": "scenarios/my_scenario/world.md",
     "character_a_file": "scenarios/my_scenario/character_a.md",
     "character_b_file": "scenarios/my_scenario/character_b.md",
     "character_a_name": "Alice",
     "character_b_name": "Bob",
     "starting_character": "a",
     "llm_config": {
       "api_base_url": "http://localhost:1234/v1",
       "model": "your-model",
       "temperature": 0.8,
       "max_tokens": 200
     },
     "conversation_config": {
       "max_turns": 10,
       "save_conversation": true,
       "output_dir": "conversations",
       "interrupt_timeout": 5
     }
   }
   ```

4. Run your scenario:
   ```bash
   python main.py scenarios/my_scenario/my_scenario_config.json
   ```

## ğŸ“š Documentation

- **[Usage Guide](docs/USAGE.md)** - Comprehensive how-to with examples
- **[Architecture](docs/ARCHITECTURE.md)** - Technical design and implementation
- **[Features](docs/FEATURES.md)** - Detailed feature documentation

## ğŸ”§ Requirements

- Python 3.7+
- OpenAI-compatible LLM server (LM Studio, Ollama, vLLM, etc.)
- Dependencies: `requests` (see `requirements.txt`)

## ğŸ¤ Contributing

This is a clean, well-documented codebase. Feel free to:
- Add new scenarios
- Enhance features
- Improve documentation
- Report issues

## ğŸ“„ License

Open source - use freely for your projects.

## ğŸ¯ Design Philosophy

**Simple & Powerful**: Built from the ground up with:
- Clean, readable code
- Comprehensive documentation
- Configuration over code
- Memory efficiency
- User control (interactive interruption)

---

**Ready to get started?** â†’ [Read the Usage Guide](docs/USAGE.md)
